# -*- coding: utf-8 -*-
"""Ensembled_PCA_Settings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XoJcdz_TI0FP4NsQIYQgGu_lJY0j6xcW
"""

# Mount google drive at /content/drive
from google.colab import drive
drive.mount('/content/drive')

# Set seeds
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import json
import pandas as pd
np.random.seed(42)
tf.random.set_seed(42)

# Fetching PCP properties of druggable and non-druggable proteins
data_file_path = "/content/drive/MyDrive/protein_props.json"
druggable_proteins_file_path = "/content/drive/MyDrive/druggable_proteins.txt"
approved_druggable_proteins_file_path = "/content/drive/MyDrive/approved_druggable_proteins.txt"

with open(data_file_path, 'r') as f:
    protein_data = json.load(f)

print("Total number of uniprot human verified proteins:", len(protein_data))

with open(druggable_proteins_file_path, 'r') as f:
    druggable_proteins = f.read().splitlines()

with open(approved_druggable_proteins_file_path, 'r') as f:
    approved_druggable_proteins = f.read().splitlines()

print("Number of druggable proteins:", len(druggable_proteins))
print("Number of approved druggable proteins:", len(approved_druggable_proteins))

properties = (pd.read_json("/content/drive/MyDrive/protein_props.json")).transpose()
is_druggable = [1 if i in druggable_proteins else 0 for i in properties.index]
is_approved_druggable = [1 if i in approved_druggable_proteins else 0 for i in properties.index]

properties["is_druggable"] = is_druggable
properties["is_approved_druggable"] = is_approved_druggable

PCP_properties = properties.copy()
amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
amino_acid_percent = {i:[] for i in amino_acids}
for i in PCP_properties['Amino Acid Percent']:
  for aa in amino_acids:
    amino_acid_percent[aa].append(i[aa])
for aa in amino_acids:
  PCP_properties = pd.concat([PCP_properties, pd.Series(amino_acid_percent[aa], index = PCP_properties.index, name = f"Amino Acid Percent {aa}")], axis = 1)

PCP_properties[f"Molar Extinction Coefficient 1"] = pd.Series([x[0] for x in PCP_properties['Molar Extinction Coefficient']], index = PCP_properties.index)
PCP_properties[f"Molar Extinction Coefficient 2"] = pd.Series([x[1] for x in PCP_properties['Molar Extinction Coefficient']], index = PCP_properties.index)

PCP_properties[f"Secondary Structure helix"] = pd.Series([x[0] for x in PCP_properties['Secondary Structure']], index = PCP_properties.index)
PCP_properties[f"Secondary Structure turn"] = pd.Series([x[1] for x in PCP_properties['Secondary Structure']], index = PCP_properties.index)
PCP_properties[f"Secondary Structure sheet"] = pd.Series([x[2] for x in PCP_properties['Secondary Structure']], index = PCP_properties.index)

PCP_properties.drop(columns = ['Amino Acid Count','Amino Acid Percent',"Molar Extinction Coefficient","Flexibility","Secondary Structure",'Sequence'], inplace = True)
PCP_properties['Sequence Length'] = PCP_properties['Sequence Length'].astype(int)
PCP_properties[['Molecular Weight', 'GRAVY', 'Isoelectric Point', 'Instability Index', 'Aromaticity', 'Charge at 7']] = PCP_properties[['Molecular Weight', 'GRAVY', 'Isoelectric Point', 'Instability Index', 'Aromaticity', 'Charge at 7']].astype(float)

with open("/content/drive/MyDrive/BDDF_Research/gdpc_encodings.json", 'r') as file:
    data = json.load(file)
gpdc_encodings = pd.DataFrame(data).transpose()

ppi = pd.read_json("/content/drive/MyDrive/ppi.json").transpose()
ppi_network = pd.read_csv("/content/drive/MyDrive/BDDF_Research/ppi_network_properties.csv")
ppi_network.index = ppi_network['Unnamed: 0']
ppi_network.drop(columns = ['Unnamed: 0'], inplace = True)
ppi = pd.concat([ppi, ppi_network], axis = 1)

glycolisation = pd.read_csv("/content/drive/MyDrive/glycosylation.csv")
glycolisation.index = glycolisation['Unnamed: 0']
glycolisation.drop(columns = ['Unnamed: 0'], inplace = True)
ptm = pd.read_csv("/content/drive/MyDrive/PTM_counts.csv")
ptm.index = ptm["Unnamed: 0"]
ptm.drop(columns = ['Unnamed: 0'], inplace = True)
ptm_counts = pd.concat([ptm, glycolisation], axis = 1)

with open("/content/drive/MyDrive/subcellular_locations2.json", 'r') as file:
    data = json.load(file)
unique_groups = set()
for entry in data.values():
    if "general" in entry:
        for general_entry in entry["general"]:
            if "group" in general_entry: unique_groups.add(general_entry["group"])

unique_groups_list = list(unique_groups)

rows = []
for protein_id in PCP_properties.index:
    row = {group: 0 for group in unique_groups_list}
    if protein_id in data:
        for entry in data[protein_id].get("general", []):
            if "group" in entry and entry["group"] in unique_groups:
                row[entry["group"]] = 1
    row["protein_id"] = protein_id
    rows.append(row)

subcellular_data = pd.DataFrame(rows).set_index("protein_id")

domains = pd.read_csv("/content/drive/MyDrive/BDDF_Research/data_top20_updated.csv")
domains.index = domains['Unnamed: 0']
domains.drop(columns = ['Unnamed: 0'], inplace = True)

flexibility = pd.read_csv("/content/drive/MyDrive/BDDF_Research/flexibility_properties.csv")
flexibility.index = flexibility['Unnamed: 0']
flexibility.drop(columns = ['Unnamed: 0'], inplace = True)

latent_data = pd.read_csv("/content/drive/MyDrive/BDDF_Research/latent_values.csv").transpose()
latent_data.columns = [f"Latent_Value_{i+1}" for i in latent_data.columns]
final_data = pd.concat([PCP_properties,gpdc_encodings, ptm_counts, ppi, subcellular_data, domains, flexibility, latent_data], axis = 1).dropna()
features_list = final_data.columns
features_list = features_list.drop(['is_druggable','is_approved_druggable'])
features_list = list(features_list)
print(features_list)
print(len(features_list))

from sklearn.decomposition import PCA
import tqdm
from tqdm import tqdm

okk = final_data[features_list]

pca_reduced_data = {}
for i in tqdm(range(1,184), desc = "PCA_transformation in progress..."):
  pca_transformer = PCA(n_components = i)
  dataset = pca_transformer.fit_transform(okk)
  pca_reduced_data[i] = dataset

#for splitting of data
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from imblearn.over_sampling import ADASYN, SMOTE
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

def get_data(x_sample, y_sample):
  return np.array(x_sample), np.array(y_sample)

def data_splitting(x_sample, y_sample, mode="default", scaler="none",reduction = "default", n_components = 183, class_size=600):

  druggable_indices = (y_sample == 1)  # Assuming 1 represents druggable
  non_druggable_indices = (y_sample == 0)  # Assuming 0 represents non-druggable

  druggable_X = x_sample[druggable_indices]
  druggable_y = y_sample[druggable_indices]

  non_druggable_X = x_sample[non_druggable_indices]
  non_druggable_y = y_sample[non_druggable_indices]

  druggable_X_remaining, druggable_X_test, druggable_y_remaining, druggable_y_test = train_test_split(druggable_X, druggable_y, test_size=class_size, random_state=123)
  non_druggable_X_remaining, non_druggable_X_test, non_druggable_y_remaining, non_druggable_y_test = train_test_split(non_druggable_X, non_druggable_y, test_size= class_size, random_state=123)

  X_test = pd.concat((druggable_X_test, non_druggable_X_test))
  y_test = pd.concat((druggable_y_test, non_druggable_y_test))
  X_train = pd.concat((druggable_X_remaining, non_druggable_X_remaining))
  y_train = pd.concat((druggable_y_remaining, non_druggable_y_remaining))
  X_train, y_train = shuffle(X_train, y_train, random_state=123)


  if scaler == "std":
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
  elif scaler == "minmax":
    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
  elif scaler == "none":
    pass

  if reduction == "pca":
    pca_transformer = PCA(n_components = n_components)
    X_train = pca_transformer.fit_transform(X_train)
    X_test = pca_transformer.transform(X_test)
  elif reduction == "default":
    pass

  if mode == "default":
    pass
  elif mode == "adasyn":
    ada = ADASYN(random_state=42)
    X_train, y_train = ada.fit_resample(X_train, y_train)
  elif mode == "smote":
    smt = SMOTE(random_state=42)
    X_train, y_train = smt.fit_resample(X_train, y_train)

  return X_train, X_test, y_train, y_test

import xgboost as xgb
import tqdm
from tqdm import tqdm
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

accuracy_metrics = {}

for i in tqdm(range(144,184), desc = "Pipeline in progress..."):
    X_train, X_test, y_train, y_test = data_splitting(final_data[features_list], final_data["is_druggable"],mode = "default",scaler = "std",reduction = "pca",n_components = i, class_size=600)
    X_train_druggable = X_train[y_train == 1]
    X_train_non_druggable = X_train[y_train == 0]

    X_train_non_druggable_partitions = np.array_split(X_train_non_druggable, int(len(X_train_non_druggable)/len(X_train_druggable)))

    xgb_models = []
    for partition in X_train_non_druggable_partitions:
      X_combined = np.concatenate((X_train_druggable, partition))
      y_combined = np.concatenate((np.ones(len(X_train_druggable)), np.zeros(len(partition))))
      xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)
      xgb_model.fit(X_combined, y_combined)
      xgb_models.append(xgb_model)

    y_preds = []
    for model in xgb_models:
      y_pred = model.predict(X_test)
      y_preds.append(y_pred)

    majority_preds = np.mean(y_preds, axis=0)
    majority_preds = np.round(majority_preds)

    accuracy_metrics[i] = {
        "accuracy_total": accuracy_score(y_test, majority_preds),
        "accuracy_druggable": accuracy_score(y_test[y_test == 1], majority_preds[y_test == 1]),
        "accuracy_non_druggable": accuracy_score(y_test[y_test == 0], majority_preds[y_test == 0]),
    }

import os

dataset_dir = "/content/drive/MyDrive/BDDF_Research/results_pca_settings/"

if not os.path.exists(dataset_dir):
    os.makedirs(dataset_dir)

target_file_path = os.path.join(dataset_dir, 'accuracy_Xgboost_Part_2.csv')
df = pd.DataFrame(accuracy_metrics).transpose()
df.to_csv(target_file_path, index=False)

print(f'File saved to {target_file_path}')

from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

accuracy_metrics_RF = {}

for i in tqdm(range(52,184), desc = "Pipeline in progress..."):
    X_train, X_test, y_train, y_test = data_splitting(final_data[features_list], final_data["is_druggable"],mode = "default",scaler = "std",reduction = "pca",n_components = i, class_size=600)
    X_train_druggable = X_train[y_train == 1]
    X_train_non_druggable = X_train[y_train == 0]

    X_train_non_druggable_partitions = np.array_split(X_train_non_druggable, int(len(X_train_non_druggable)/len(X_train_druggable)))

    dt_models = []
    for partition in X_train_non_druggable_partitions:
      X_combined = np.concatenate((X_train_druggable, partition))
      y_combined = np.concatenate((np.ones(len(X_train_druggable)), np.zeros(len(partition))))
      dt = DecisionTreeClassifier(random_state=42)
      dt.fit(X_combined, y_combined)
      dt_models.append(dt)

    y_preds = []
    for model in dt_models:
      y_pred = model.predict(X_test)
      y_preds.append(y_pred)

    majority_preds = np.mean(y_preds, axis=0)
    majority_preds = np.round(majority_preds)

    accuracy_metrics_RF[i] = {
        "accuracy_total": accuracy_score(y_test, majority_preds),
        "accuracy_druggable": accuracy_score(y_test[y_test == 1], majority_preds[y_test == 1]),
        "accuracy_non_druggable": accuracy_score(y_test[y_test == 0], majority_preds[y_test == 0]),
    }

import os

dataset_dir = "/content/drive/MyDrive/BDDF_Research/results_pca_settings/"

if not os.path.exists(dataset_dir):
    os.makedirs(dataset_dir)

target_file_path = os.path.join(dataset_dir, 'accuracy_Decision_Tree_Part_2.csv')
df = pd.DataFrame(accuracy_metrics_RF).transpose()
df.to_csv(target_file_path, index=False)

print(f'File saved to {target_file_path}')

from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier

accuracy_metrics_MLP = {}

for i in tqdm(range(1,184), desc = "Pipeline in progress..."):
    X_train, X_test, y_train, y_test = data_splitting(final_data[features_list], final_data["is_druggable"],mode = "default",scaler = "std",reduction = "pca",n_components = i, class_size=600)
    X_train_druggable = X_train[y_train == 1]
    X_train_non_druggable = X_train[y_train == 0]

    X_train_non_druggable_partitions = np.array_split(X_train_non_druggable, int(len(X_train_non_druggable)/len(X_train_druggable)))

    dt_models = []
    for partition in X_train_non_druggable_partitions:
      X_combined = np.concatenate((X_train_druggable, partition))
      y_combined = np.concatenate((np.ones(len(X_train_druggable)), np.zeros(len(partition))))
      mlp = MLPClassifier(hidden_layer_sizes=(64,), max_iter=100, alpha=0.001, solver='adam', random_state=42)
      mlp.fit(X_combined, y_combined)
      dt_models.append(mlp)

    y_preds = []
    for model in dt_models:
      y_pred = model.predict(X_test)
      y_preds.append(y_pred)

    majority_preds = np.mean(y_preds, axis=0)
    majority_preds = np.round(majority_preds)

    accuracy_metrics_MLP[i] = {
        "accuracy_total": accuracy_score(y_test, majority_preds),
        "accuracy_druggable": accuracy_score(y_test[y_test == 1], majority_preds[y_test == 1]),
        "accuracy_non_druggable": accuracy_score(y_test[y_test == 0], majority_preds[y_test == 0]),
    }

import os

dataset_dir = "/content/drive/MyDrive/BDDF_Research/results_pca_settings/"

if not os.path.exists(dataset_dir):
    os.makedirs(dataset_dir)

target_file_path = os.path.join(dataset_dir, 'accuracy_MLP.csv')
df = pd.DataFrame(accuracy_metrics_MLP).transpose()
df.to_csv(target_file_path, index=False)

print(f'File saved to {target_file_path}')

